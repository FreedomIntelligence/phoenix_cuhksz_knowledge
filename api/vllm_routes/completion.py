import asyncio
import time
import traceback
import uuid
from functools import partial
from typing import AsyncIterator, Tuple

import anyio
from fastapi import APIRouter, Depends
from fastapi import Request
from loguru import logger
from openai.types.completion import Completion
from openai.types.completion_choice import CompletionChoice, Logprobs
from openai.types.completion_usage import CompletionUsage
from sse_starlette import EventSourceResponse
from vllm.model_executor.guided_decoding import get_guided_decoding_logits_processor
from vllm.outputs import RequestOutput
from vllm.sampling_params import SamplingParams

from api.core.vllm_engine import VllmEngine
from api.models import GENERATE_ENGINE
from api.utils.compat import model_dump
from api.utils.protocol import CompletionCreateParams
from api.utils.request import (
    handle_request,
    get_event_publisher,
    check_api_key,
)

completion_router = APIRouter()


def get_engine():
    yield GENERATE_ENGINE


def parse_prompt_format(prompt) -> Tuple[bool, list]:
    # get the prompt, openai supports the following
    # "a string, array of strings, array of tokens, or array of token arrays."
    prompt_is_tokens = False
    prompts = [prompt]  # case 1: a string
    if isinstance(prompt, list):
        if len(prompt) == 0:
            raise ValueError("please provide at least one prompt")
        elif isinstance(prompt[0], str):
            prompt_is_tokens = False
            prompts = prompt  # case 2: array of strings
        elif isinstance(prompt[0], int):
            prompt_is_tokens = True
            prompts = [prompt]  # case 3: array of tokens
        elif isinstance(prompt[0], list) and isinstance(prompt[0][0], int):
            prompt_is_tokens = True
            prompts = prompt  # case 4: array of token arrays
        else:
            raise ValueError(
                "prompt must be a string, array of strings, array of tokens, or array of token arrays"
            )
    return prompt_is_tokens, prompts


def merge_async_iterators(*iterators):
    """Merge multiple asynchronous iterators into a single iterator.

    This method handle the case where some iterators finish before others.
    When it yields, it yields a tuple (i, item) where i is the index of the
    iterator that yields the item.
    """
    queue = asyncio.Queue()

    finished = [False] * len(iterators)

    async def producer(i, iterator):
        try:
            async for item in iterator:
                await queue.put((i, item))
        except Exception as e:
            await queue.put(e)
        finished[i] = True

    _tasks = [
        asyncio.create_task(producer(i, iterator))
        for i, iterator in enumerate(iterators)
    ]

    async def consumer():
        while not all(finished) or not queue.empty():
            item = await queue.get()
            if isinstance(item, Exception):
                raise item
            yield item
        await asyncio.gather(*_tasks)

    return consumer()


@completion_router.post("/completions", dependencies=[Depends(check_api_key)])
async def create_completion(
    request: CompletionCreateParams,
    raw_request: Request,
    engine: VllmEngine = Depends(get_engine),
):
    """Completion API similar to OpenAI's API.

    See https://platform.openai.com/docs/api-reference/completions/create
    for the API specification. This API mimics the OpenAI Completion API.
    """
    request.max_tokens = request.max_tokens or 128
    request = await handle_request(request, engine.prompt_adapter.stop, chat=False)

    if isinstance(request.prompt, list):
        request.prompt = request.prompt[0]

    params = model_dump(request, exclude={"prompt"})
    params.update(dict(prompt_or_messages=request.prompt))
    logger.debug(f"==== request ====\n{params}")

    request_id: str = f"cmpl-{str(uuid.uuid4())}"
    # Schedule the request and get the result generator.
    generators = []
    num_prompts = 1
    try:
        include = {
            "n",
            "presence_penalty",
            "frequency_penalty",
            "temperature",
            "top_p",
            "repetition_penalty",
            "min_p",
            "best_of",
            "ignore_eos",
            "use_beam_search",
            "skip_special_tokens",
            "spaces_between_special_tokens",
        }
        kwargs = model_dump(request, include=include)
        sampling_params = SamplingParams(
            stop=request.stop or [],
            stop_token_ids=request.stop_token_ids or [],
            max_tokens=request.max_tokens,
            **kwargs,
        )
        lora_request = engine._maybe_get_lora(request.model)
        guided_decode_logits_processor = (
            await get_guided_decoding_logits_processor(
                request,
                engine.tokenizer,
            )
        )
        if guided_decode_logits_processor:
            sampling_params.logits_processors = sampling_params.logits_processors or []
            sampling_params.logits_processors.append(guided_decode_logits_processor)

        prompt_is_tokens, prompts = parse_prompt_format(request.prompt)
        num_prompts = len(prompts)

        for i, prompt in enumerate(prompts):
            if prompt_is_tokens:
                input_ids = engine.convert_to_inputs(token_ids=prompt, max_tokens=request.max_tokens)
            else:
                input_ids = engine.convert_to_inputs(prompt=prompt, max_tokens=request.max_tokens)

            generators.append(
                engine.model.generate(
                    prompt,
                    sampling_params,
                    f"{request_id}-{i}",
                    prompt_token_ids=input_ids,
                    lora_request=lora_request
                )
            )
    except ValueError as e:
        traceback.print_exc()

    result_generator: AsyncIterator[Tuple[int, RequestOutput]] = merge_async_iterators(*generators)

    if request.stream:
        iterator = create_completion_stream(
            engine, result_generator, request, request_id, num_prompts
        )
        send_chan, recv_chan = anyio.create_memory_object_stream(10)
        return EventSourceResponse(
            recv_chan,
            data_sender_callable=partial(
                get_event_publisher,
                request=raw_request,
                inner_send_chan=send_chan,
                iterator=iterator,
            ),
        )
    else:
        # Non-streaming response
        final_res_batch = [None] * num_prompts
        async for i, res in result_generator:
            if await raw_request.is_disconnected():
                # Abort the request if the client disconnects.
                await engine.model.abort(f"{request_id}-{i}")
            final_res_batch[i] = res

        choices = []
        num_prompt_tokens = 0
        num_generated_tokens = 0
        for final_res in final_res_batch:
            final_res: RequestOutput
            prompt_token_ids = final_res.prompt_token_ids
            prompt_logprobs = final_res.prompt_logprobs
            prompt_text = final_res.prompt

            for output in final_res.outputs:
                if request.echo and request.max_tokens == 0:
                    token_ids = prompt_token_ids
                    top_logprobs = prompt_logprobs
                    output_text = prompt_text
                elif request.echo and request.max_tokens > 0:
                    token_ids = prompt_token_ids + output.token_ids
                    top_logprobs = prompt_logprobs + output.logprobs
                    output_text = prompt_text + output.text
                else:
                    token_ids = output.token_ids
                    top_logprobs = output.logprobs
                    output_text = output.text

                if request.logprobs is not None:
                    logprobs = engine.create_logprobs(
                        token_ids=token_ids,
                        top_logprobs=top_logprobs,
                        num_output_top_logprobs=request.logprobs,
                    )
                else:
                    logprobs = None

                choice = CompletionChoice(
                    index=len(choices),
                    text=output_text,
                    finish_reason=output.finish_reason,
                    logprobs=logprobs,
                )
                choices.append(choice)

                num_prompt_tokens += len(prompt_token_ids)
                num_generated_tokens += sum(len(output.token_ids) for output in final_res.outputs)

        usage = CompletionUsage(
            prompt_tokens=num_prompt_tokens,
            completion_tokens=num_generated_tokens,
            total_tokens=num_prompt_tokens + num_generated_tokens,
        )

        return Completion(
            id=request_id,
            choices=choices,
            created=int(time.time()),
            model=request.model,
            object="text_completion",
            usage=usage,
        )


async def create_completion_stream(
    engine: VllmEngine,
    generator: AsyncIterator,
    request: CompletionCreateParams,
    request_id: str,
    num_prompts: int,
) -> AsyncIterator:
    previous_texts = [""] * request.n * num_prompts
    previous_num_tokens = [0] * request.n * num_prompts
    has_echoed = [False] * request.n * num_prompts
    try:
        async for prompt_idx, res in generator:
            res: RequestOutput
            for output in res.outputs:
                i = output.index + prompt_idx * request.n
                output.text = output.text.replace("ï¿½", "")

                if request.echo and request.max_tokens == 0:
                    # only return the prompt
                    delta_text = res.prompt
                    delta_token_ids = res.prompt_token_ids
                    top_logprobs = res.prompt_logprobs
                    has_echoed[i] = True
                elif request.echo and request.max_tokens > 0 and not has_echoed[i]:
                    # echo the prompt and first token
                    delta_text = res.prompt + output.text
                    delta_token_ids = res.prompt_token_ids + output.token_ids
                    top_logprobs = res.prompt_logprobs + output.logprobs or []
                    has_echoed[i] = True
                else:
                    # return just the delta
                    delta_text = output.text[len(previous_texts[i]):]
                    delta_token_ids = output.token_ids[previous_num_tokens[i]:]
                    top_logprobs = output.logprobs[previous_num_tokens[i]:] if output.logprobs else None

                if request.logprobs is not None:
                    assert top_logprobs is not None, (
                        "top_logprobs must be provided when logprobs "
                        "is requested")
                    logprobs = engine.create_logprobs(
                        token_ids=delta_token_ids,
                        top_logprobs=top_logprobs,
                        num_output_top_logprobs=request.logprobs,
                        initial_text_offset=len(previous_texts[i]),
                    )
                else:
                    logprobs = None

                previous_texts[i] = output.text
                previous_num_tokens[i] = len(output.token_ids)

                choice = CompletionChoice(
                    index=i,
                    text=delta_text,
                    finish_reason="stop",
                    logprobs=logprobs,
                )
                yield Completion(
                    id=request_id,
                    choices=[choice],
                    created=int(time.time()),
                    model=request.model,
                    object="text_completion",
                )

                if output.finish_reason is not None:
                    if request.logprobs is not None:
                        logprobs = Logprobs(
                            text_offset=[], token_logprobs=[], tokens=[], top_logprobs=[]
                        )
                    else:
                        logprobs = None

                    choice = CompletionChoice(
                        index=i,
                        text=delta_text,
                        finish_reason="stop",
                        logprobs=logprobs,
                    )
                    yield Completion(
                        id=request_id,
                        choices=[choice],
                        created=int(time.time()),
                        model=request.model,
                        object="text_completion",
                    )
    except:
        traceback.print_exc()
